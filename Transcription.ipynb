{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T15:40:09.358504Z",
     "iopub.status.busy": "2025-03-20T15:40:09.358166Z",
     "iopub.status.idle": "2025-03-20T15:40:09.453749Z",
     "shell.execute_reply": "2025-03-20T15:40:09.453121Z",
     "shell.execute_reply.started": "2025-03-20T15:40:09.358477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import noisereduce as nr\n",
    "import datetime\n",
    "\n",
    "hf_token = \"hf_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T15:39:09.317303Z",
     "iopub.status.busy": "2025-03-20T15:39:09.316994Z",
     "iopub.status.idle": "2025-03-20T15:39:09.326223Z",
     "shell.execute_reply": "2025-03-20T15:39:09.325360Z",
     "shell.execute_reply.started": "2025-03-20T15:39:09.317280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_audio(video_path:str, output_audio_path=\"temp_audio.wav\"):\n",
    "    \"xtracts audio.wav frtom video file\"\n",
    "    try:\n",
    "        video = AudioSegment.from_file(video_path)\n",
    "        audio = video.split_to_mono()[0]  \n",
    "        audio.export(output_audio_path, format=\"wav\")\n",
    "        return output_audio_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "def speakers_with_timestamp(audio_path:str):\n",
    "    \"produces timestamped speaker information from audio file\"\n",
    "    from pyannote.audio import Pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=hf_token)\n",
    "    diarization = pipeline(audio_path)\n",
    "    speaker_timestamps=[]\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_timestamps.append({'speaker':speaker,'start':round(turn.start,2),'end':round(turn.end,2)})\n",
    "    return speaker_timestamps\n",
    "\n",
    "\n",
    "def preprocess_(audio_path:str,speaker_timestamps:list):\n",
    "    \"creates a dataset for transcribing the spoken parts based on the speaker timestamps\"\n",
    "    y,sr=librosa.load(audio_path)\n",
    "    y=y.astype(np.float64)\n",
    "    y = nr.reduce_noise(y=y, sr=sr) \n",
    "    y = y / np.max(np.abs(y))\n",
    "    audio_samples=[]\n",
    "    for item in speaker_timestamps:\n",
    "        sample=y[round(item['start']*sr):round(item['end']*sr)+1]\n",
    "        audio_samples.append({\"array\":sample,'sampling_rate':sr})\n",
    "    return audio_samples\n",
    "\n",
    "\n",
    "def transcrsibe(video_path:str,language:str,translate:bool):\n",
    "    \"\"\"adds transcribed text information to the speaker with timestamp\n",
    "    video_path: input video path\n",
    "    translate: translates to english if set to True else in - language\n",
    "    language: language to trancribe in\n",
    "    \"\"\"\n",
    "    audio_path=extract_audio(video_path)\n",
    "    sp_timstamp=speakers_with_timestamp(audio_path)\n",
    "    samples_=preprocess_(audio_path,sp_timstamp)\n",
    "    for idx,sample_ in enumerate(samples_):\n",
    "        generate_kwargs={\"language\": language,\"task\": \"translate\"} if translate  else {\"language\": language}\n",
    "        result = pipe(sample_,return_timestamps=True,generate_kwargs=generate_kwargs)\n",
    "        sp_timstamp[idx].update({'text':result['text']})\n",
    "    return sp_timstamp\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"time formatting for srt\"\n",
    "    td = datetime.timedelta(seconds=seconds)\n",
    "    hrs, remainder = divmod(td.seconds, 3600)\n",
    "    mins, secs = divmod(remainder, 60)\n",
    "    millis = td.microseconds // 1000\n",
    "    return f\"{hrs:02}:{mins:02}:{secs:02},{millis:03}\"\n",
    "\n",
    "\n",
    "\n",
    "def out_file(data,name):\n",
    "    \"writes the informatison into srt file and saves it\"\n",
    "    with open(name, 'w') as file:\n",
    "        for idx, item in enumerate(data, start=1):\n",
    "            start_time = format_time(item['start'])\n",
    "            end_time = format_time(item['end'])\n",
    "            file.write(f\"{idx}\\n{start_time} --> {end_time}\\n{item['speaker']}: {item['text']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T16:00:33.239178Z",
     "iopub.status.busy": "2025-03-20T16:00:33.238741Z",
     "iopub.status.idle": "2025-03-20T16:02:11.058982Z",
     "shell.execute_reply": "2025-03-20T16:02:11.058098Z",
     "shell.execute_reply.started": "2025-03-20T16:00:33.239154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 1min 16s, total: 2min 59s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# running inference\n",
    "\n",
    "video_path='/kaggle/input/video-examples-data/hindi_multi.mp4'  #any input video path\n",
    "out=transcrsibe(video_path=video_path,language='hindi',translate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T16:02:26.632946Z",
     "iopub.status.busy": "2025-03-20T16:02:26.632632Z",
     "iopub.status.idle": "2025-03-20T16:02:26.638282Z",
     "shell.execute_reply": "2025-03-20T16:02:26.637435Z",
     "shell.execute_reply.started": "2025-03-20T16:02:26.632924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,080 --> 00:00:01,350\n",
      "SPEAKER_00:  Ravi, bring me a cup of coffee.\n",
      "\n",
      "2\n",
      "00:00:01,870 --> 00:00:02,260\n",
      "SPEAKER_00:  So, what?\n",
      "\n",
      "3\n",
      "00:00:03,860 --> 00:00:04,170\n",
      "SPEAKER_01:  No.\n",
      "\n",
      "4\n",
      "00:00:04,920 --> 00:00:05,630\n",
      "SPEAKER_01:  Hey Shetty!\n",
      "\n",
      "5\n",
      "00:00:07,050 --> 00:00:08,100\n",
      "SPEAKER_01:  You have two options.\n",
      "\n",
      "6\n",
      "00:00:09,030 --> 00:00:09,720\n",
      "SPEAKER_01:  Leave us.\n",
      "\n",
      "7\n",
      "00:00:10,800 --> 00:00:13,780\n",
      "SPEAKER_01:  You'll get so much money that you won't have a place to stay in this police station.\n",
      "\n",
      "8\n",
      "00:00:15,200 --> 00:00:15,220\n",
      "SPEAKER_01:  Welcome to our Channel.\n",
      "\n",
      "9\n",
      "00:00:15,220 --> 00:00:15,250\n",
      "SPEAKER_02:  Welcome to our Channel.\n",
      "\n",
      "10\n",
      "00:00:15,250 --> 00:00:16,160\n",
      "SPEAKER_01:  And the second option\n",
      "\n",
      "11\n",
      "00:00:16,160 --> 00:00:16,180\n",
      "SPEAKER_02:  Welcome to our Channel.\n",
      "\n",
      "12\n",
      "00:00:17,010 --> 00:00:17,020\n",
      "SPEAKER_01:  Thank you for watching.\n",
      "\n",
      "13\n",
      "00:00:17,020 --> 00:00:18,580\n",
      "SPEAKER_02:  You won't be able to save this police station.\n",
      "\n",
      "14\n",
      "00:00:20,480 --> 00:00:21,510\n",
      "SPEAKER_02:  You don't know her.\n",
      "\n",
      "15\n",
      "00:00:26,220 --> 00:00:26,950\n",
      "SPEAKER_00:  Threatening me?\n",
      "\n",
      "16\n",
      "00:00:27,570 --> 00:00:29,190\n",
      "SPEAKER_00:  You are talking about Danger Langa, right?\n",
      "\n",
      "17\n",
      "00:00:30,520 --> 00:00:31,110\n",
      "SPEAKER_00:  Who is he?\n",
      "\n",
      "18\n",
      "00:00:33,090 --> 00:00:33,850\n",
      "SPEAKER_00:  I know.\n",
      "\n",
      "19\n",
      "00:00:34,190 --> 00:00:35,350\n",
      "SPEAKER_00:  The real danger\n",
      "\n",
      "20\n",
      "00:00:35,820 --> 00:00:36,950\n",
      "SPEAKER_00:  is waiting for you.\n",
      "\n",
      "21\n",
      "00:00:37,070 --> 00:00:39,750\n",
      "SPEAKER_01:  Every officer who wears uniform thinks he is Singham.\n",
      "\n",
      "22\n",
      "00:00:39,770 --> 00:00:40,800\n",
      "SPEAKER_00:  Arararara\n",
      "\n",
      "23\n",
      "00:00:41,510 --> 00:00:42,390\n",
      "SPEAKER_00:  No, dad.\n",
      "\n",
      "24\n",
      "00:00:43,260 --> 00:00:45,360\n",
      "SPEAKER_00:  He is our Guruji.\n",
      "\n",
      "25\n",
      "00:00:46,340 --> 00:00:47,280\n",
      "SPEAKER_00:  I am not a Singham.\n",
      "\n",
      "26\n",
      "00:00:48,560 --> 00:00:50,280\n",
      "SPEAKER_00:  I am Lady Singham.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# saving the file and reding it in to seee the out\n",
    "\n",
    "out_file(out,'transcript.srt')\n",
    "\n",
    "with open('/kaggle/working/transcript.srt', 'r') as file:\n",
    "    subtitles = file.read()\n",
    "print(subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11496519,
     "datasetId": 6914298,
     "sourceId": 11105216,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
